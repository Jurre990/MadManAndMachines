{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import time\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocessing as mp\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization of dataset\n",
    "multiprocessing in seperate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#read data\n",
    "def load():\n",
    "    df = pd.read_csv('C:\\\\Users\\\\vd00r\\\\OneDrive\\\\Documenten\\\\GitHub\\\\MadManAndMachines\\\\pythonProject\\\\text_data.csv', sep=',')\n",
    "    #turn string to list\n",
    "    for i, id in enumerate(df['codes']):\n",
    "        df['codes'][i] = eval(id)\n",
    "    #drop data\n",
    "    df = df.drop(['Unnamed: 0'],axis = 1)\n",
    "    return(df.to_numpy())\n",
    "\n",
    "#multiprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "cpu_c = os.cpu_count()//2 \n",
    "\n",
    "def transform(fw):\n",
    "    print(fw[0])\n",
    "    tok = tokenizer.encode_plus(fw[0])\n",
    "    return tok,fw[1]\n",
    "\n",
    "def proc(fw):\n",
    "    with mp.Pool(cpu_c) as pool:\n",
    "        tokenized_text = []\n",
    "        labels = []\n",
    "        for data in pool.imap(transform,fw,chunksize=16):\n",
    "            tokenized_text.append(data[0])\n",
    "            labels.append(data[1])\n",
    "        return tokenized_text,labels\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = load()\n",
    "    #print(df)\n",
    "    a = proc(df)\n",
    "    #print(a[0]['input_ids'])\n",
    "    d = {\"inputs\": a[0],\"labels\": a[1]} \n",
    "    array = pd.DataFrame(data = d).to_csv('dataset.csv',index=False)\n",
    "    print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 27]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('dataset.csv', sep=',', converters={'labels': pd.eval})\n",
    "print(df['labels'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class political_ads_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('dataset.csv', sep=',', converters={'labels': pd.eval})\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.data['inputs'].iloc[idx]\n",
    "        input = ast.literal_eval(input)['input_ids']\n",
    "        input = torch.tensor(input)\n",
    "        input = F.pad(input,(0,(512-input.shape[0])),\"constant\",0)\n",
    "        label = self.data['labels'].iloc[idx]\n",
    "        return input,label[0]\n",
    "    def downsize(self):\n",
    "        for i, value in enumerate(self.data['labels']):\n",
    "            self.data['labels'][i] = value[0]\n",
    "        self.data = self.data.drop(self.data[self.data['labels'].eq(0)].sample(840).index)\n",
    "        self.data = self.data.drop(self.data[self.data['labels'].eq(10)].sample(70).index)\n",
    "        self.data = self.data.reset_index()\n",
    "        return self.data\n",
    "    def data_return(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 inputs labels\n",
      "1     {'input_ids': [101, 24890, 8231, 25370, 16716,...     21\n",
      "3     {'input_ids': [101, 2777, 112, 188, 1103, 108,...     29\n",
      "5     {'input_ids': [101, 24890, 8231, 25370, 16716,...     13\n",
      "6     {'input_ids': [101, 157, 2069, 25810, 2101, 11...     30\n",
      "9     {'input_ids': [101, 1135, 1110, 24819, 2924, 1...     24\n",
      "...                                                 ...    ...\n",
      "1079  {'input_ids': [101, 3561, 119, 2103, 1107, 170...      4\n",
      "1084  {'input_ids': [101, 138, 8499, 118, 17750, 346...     29\n",
      "1093  {'input_ids': [101, 1409, 1809, 117, 8499, 120...     29\n",
      "1108  {'input_ids': [101, 1192, 787, 1396, 7348, 115...     28\n",
      "1110  {'input_ids': [101, 26385, 136, 23114, 106, 17...      6\n",
      "\n",
      "[212 rows x 2 columns]\n",
      "labels\n",
      "10    25\n",
      "0     23\n",
      "29    20\n",
      "28    15\n",
      "11    14\n",
      "21    13\n",
      "24    13\n",
      "6     11\n",
      "22    11\n",
      "5      9\n",
      "1      9\n",
      "3      8\n",
      "27     7\n",
      "13     7\n",
      "30     6\n",
      "16     5\n",
      "2      5\n",
      "25     3\n",
      "20     2\n",
      "8      2\n",
      "4      2\n",
      "12     1\n",
      "7      1\n",
      "Name: count, dtype: int64 10\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv', sep=',', converters={'labels': pd.eval})\n",
    "\n",
    "for i, value in enumerate(data['labels']):\n",
    "    data['labels'][i] = value[0]\n",
    "    #print(value[0])\n",
    "data = data.drop(data[data['labels'].eq(0)].sample(840).index)\n",
    "data = data.drop(data[data['labels'].eq(10)].sample(70).index)\n",
    "print(data)\n",
    "count = data['labels'].value_counts()\n",
    "print(count,count.index[0])\n",
    "#calculate index inverse frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "tensor([0.9455, 0.9787, 0.9882, 0.9810, 0.9953, 0.9787, 0.9739, 0.9976, 0.9953,\n",
      "        0.0000, 0.9408, 0.9668, 0.9976, 0.9834, 0.0000, 0.0000, 0.9882, 0.0000,\n",
      "        0.0000, 0.0000, 0.9953, 0.9692, 0.9739, 0.0000, 0.9692, 0.9929, 0.0000,\n",
      "        0.9834, 0.9645, 0.9526, 0.9858, 0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "length = 422\n",
    "def get_ratio(value):\n",
    "    return (length-value)/length\n",
    "\n",
    "weights = np.zeros((32,))\n",
    "print(weights.shape)\n",
    "for i, value in enumerate(count):\n",
    "    loc = count.index[i]\n",
    "    weights[loc] = get_ratio(value)\n",
    "\n",
    "weight = torch.tensor(weights)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    n_embd = 768\n",
    "    n_classes = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_classifier(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model_bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "        self.lin = nn.Linear(config.n_embd,config.n_classes)\n",
    "        #self.softmax = nn.softmax()\n",
    "    def forward(self,x):\n",
    "        x = self.model_bert(x)\n",
    "        x = self.lin(x[1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Gebruiker/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "model = bert_classifier(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "bert_classifier                                              [1, 32]                   --\n",
       "├─BertModel: 1-1                                             [1, 768]                  --\n",
       "│    └─BertEmbeddings: 2-1                                   [1, 512, 768]             --\n",
       "│    │    └─Embedding: 3-1                                   [1, 512, 768]             23,440,896\n",
       "│    │    └─Embedding: 3-2                                   [1, 512, 768]             1,536\n",
       "│    │    └─Embedding: 3-3                                   [1, 512, 768]             393,216\n",
       "│    │    └─LayerNorm: 3-4                                   [1, 512, 768]             1,536\n",
       "│    │    └─Dropout: 3-5                                     [1, 512, 768]             --\n",
       "│    └─BertEncoder: 2-2                                      [1, 512, 768]             --\n",
       "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
       "│    └─BertPooler: 2-3                                       [1, 768]                  --\n",
       "│    │    └─Linear: 3-7                                      [1, 768]                  590,592\n",
       "│    │    └─Tanh: 3-8                                        [1, 768]                  --\n",
       "├─Linear: 1-2                                                [1, 32]                   24,608\n",
       "==============================================================================================================\n",
       "Total params: 109,506,848\n",
       "Trainable params: 109,506,848\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 109.51\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 427.83\n",
       "Params size (MB): 438.03\n",
       "Estimated Total Size (MB): 865.85\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_data=torch.ones([1,512],dtype=torch.int32).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----TRAINING-HYPERPARAMETERS-----#\n",
    "max_lr = 5e-6\n",
    "min_lr = max_lr * 0.01\n",
    "warmup_steps = 70\n",
    "max_steps = 700\n",
    "theta = 0.7\n",
    "#-----TRAINING-HYPERPARAMETERS-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#-----BATCH-SIZE/GRADIENT-ACCUMILATION-----#\n",
    "dataset = political_ads_Dataset()\n",
    "weight_data = dataset.data_return()\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
    "print(len(test_set))\n",
    "batch_size = 64\n",
    "B = 32\n",
    "grad_accum_steps = batch_size // B\n",
    "train_loader = DataLoader(train_set, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=B, shuffle=True)\n",
    "print(grad_accum_steps)\n",
    "#-----BATCH-SIZE/GRADIENT-ACCUMILATION-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "[0]             863\n",
      "[10]             74\n",
      "[29]             18\n",
      "[28]             14\n",
      "[11]             13\n",
      "               ... \n",
      "[10, 24, 29]      1\n",
      "[10, 13]          1\n",
      "[8, 10, 16]       1\n",
      "[24, 29]          1\n",
      "[10, 27, 31]      1\n",
      "Name: count, Length: 63, dtype: int64\n",
      "(32,)\n",
      "tensor([0.0065, 0.3840, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000,\n",
      "        0.0000, 0.7000, 0.7000, 0.7000, 0.7000, 0.0000, 0.0000, 0.7000, 0.0000,\n",
      "        0.0000, 0.0000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.0000,\n",
      "        0.7000, 0.7000, 0.7000, 0.7000, 0.7000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "count = weight_data['labels'].value_counts()\n",
    "print(count)\n",
    "\n",
    "length = len(dataset)\n",
    "def get_ratio(value):\n",
    "    return (value)/length\n",
    "\n",
    "weights = np.zeros((32,))\n",
    "print(weights.shape)\n",
    "for i, value in enumerate(count):\n",
    "    loc = count.index[i]\n",
    "    weights[loc] = min(1/(100*get_ratio(value)+1),0.7)\n",
    "weight = torch.tensor(weights)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train_features, train_labels = next(iter(test_loader))\n",
    "    print(train_features.shape)\n",
    "    print(train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----LEARNING-RATE-SCHEDULER-----#\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay = it-warmup_steps\n",
    "    return max_lr - decay * (max_lr/(max_steps-warmup_steps))\n",
    "#-----LEARNING-RATE-SCHEDULER-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----PARAMETER-SETUP-----#\n",
    "for param in model.parameters():\n",
    "     param.requires_grad = False\n",
    "for param in model.model_bert.pooler.parameters():\n",
    "     param.requires_grad = True\n",
    "for param in model.lin.parameters():\n",
    "     param.requires_grad = True\n",
    "#-----PARAMETER-SETUP-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for param in model.named_parameters():\n",
    "     layer_names.append(param[0])\n",
    "layer_names.reverse()\n",
    "#print(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----GRADUAL-FREEZING----#\n",
    "class gradual_freezing():\n",
    "    def __init__(self):\n",
    "        self.unlock_ratio = 0.083\n",
    "        self.i = 12\n",
    "    def check_unfreeze(self,it):\n",
    "        ratio = it/max_steps\n",
    "        #print(ratio)\n",
    "        #print(self.unlock_ratio)\n",
    "        if ratio > self.unlock_ratio and self.i != 0:\n",
    "            self.unlock_ratio = self.unlock_ratio +  0.083\n",
    "            self.i = self.i - 1\n",
    "            print(self.i)\n",
    "            for param in model.model_bert.encoder.layer[self.i].parameters():\n",
    "                param.requires_grad = True\n",
    "                #print(param[0])\n",
    "            return True\n",
    "        return False\n",
    "#-----GRADUAL-FREEZING-----#\n",
    "a = gradual_freezing()\n",
    "#for g in range(2000):\n",
    "    #a.check_unfreeze(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----DISCRIMINATIVE-FINE-TUNING-----#\n",
    "def disc_fine_tuning(lr):\n",
    "    parameters = []\n",
    "    list1 = []\n",
    "    previous = layer_names[2].split('.')[2]\n",
    "    lr_disc = lr\n",
    "    for idx,name in enumerate(layer_names):\n",
    "        if name.split(\".\")[0] == \"lin\":\n",
    "            for p in model.named_parameters():\n",
    "                if p[0] == name and p[1].requires_grad:\n",
    "                    parameters.append({ \"params\":p[1] ,\"lr\":lr})\n",
    "        if name.split(\".\")[1] == \"pooler\":\n",
    "            for p in model.named_parameters():\n",
    "                if p[0] == name and p[1].requires_grad:\n",
    "                    parameters.append({ \"params\":p[1] ,\"lr\":lr_disc})\n",
    "                    list1.append(p[0])\n",
    "        if name.split(\".\")[1] == \"embeddings\":\n",
    "            for p in model.named_parameters():\n",
    "                if p[0] == name and p[1].requires_grad:\n",
    "                    parameters.append({ \"params\":p[1] ,\"lr\":lr_disc})\n",
    "        if name.split(\".\")[1] == \"encoder\":\n",
    "                current = name.split('.')[3]\n",
    "                if current != previous:\n",
    "                    lr_disc = theta*lr_disc\n",
    "                previous = current\n",
    "                for p in model.named_parameters():\n",
    "                    if p[0] == name and p[1].requires_grad:\n",
    "                        parameters.append({ \"params\":p[1] ,\"lr\":lr_disc})\n",
    "                        list1.append(p[0])\n",
    "    return parameters, list1\n",
    "#-----DISCRIMINATIVE-FINE-TUNING-----#\n",
    "#parameter,list1 = disc_fine_tuning(1)\n",
    "#print(parameter)\n",
    "#optimizer = torch.optim.Adam(parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614400 800\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "#-----OPTIMIZER-----#\n",
    "def configure_optimizers(model, weight_decay, learning_rate, device):\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {'params': decay_params, 'weight_decay': weight_decay},\n",
    "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params) \n",
    "    print(num_decay_params,num_nodecay_params)\n",
    "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device == \"cuda\"\n",
    "    print(f\"using fused AdamW: {use_fused}\")\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, fused=use_fused)\n",
    "    return optimizer  \n",
    "  \n",
    "optimizer = configure_optimizers(model = model, weight_decay=0.05, learning_rate=6e-4, device=device)\n",
    "#-----OPTIMIZER-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----LOSS-FUNCTION-----#\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, ignore_index=-100, reduction='mean'):\n",
    "        super().__init__()\n",
    "        # use standard CE loss without reducion as basis\n",
    "        self.CE = nn.CrossEntropyLoss(reduction='none', ignore_index=ignore_index)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        '''\n",
    "        input (B, N)\n",
    "        target (B)\n",
    "        '''\n",
    "        minus_logpt = self.CE(input, target)\n",
    "        pt = torch.exp(-minus_logpt) # don't forget the minus here\n",
    "        focal_loss = (1-pt)**self.gamma * minus_logpt\n",
    "\n",
    "        # apply class weights\n",
    "        if self.alpha != None:\n",
    "            focal_loss *= self.alpha.gather(0, target)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            focal_loss = focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            focal_loss = focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "criterion = FocalLoss(alpha=weight.to(device),gamma=5)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#-----LOSS-FUNCTION-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----MISC-----#\n",
    "checkpoint = 0\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#-----MISC-----#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  0,  0,  0, 10,  0,  4,  2,  0, 10,  0, 27,  0, 30,  0,  0,  0,  0,\n",
      "         0,  0, 21,  0,  0,  0,  0,  0,  0,  0, 11,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "tensor([30, 23, 30, 23, 23, 30, 23, 26, 30, 23, 30, 30, 30, 26, 30, 30, 26, 23,\n",
      "        30, 30,  7, 23, 30, 30, 23, 30, 30, 23, 30, 26, 23, 30],\n",
      "       device='cuda:0')\n",
      "| step = 0 | time = 2.6951s | lr = 7.1429e-08 | loss = 0.569364 |norm = 1.1787| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 1 | time = 0.3536s | lr = 1.4286e-07 | loss = 0.344757 |norm = 1.6272| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 2 | time = 0.1885s | lr = 2.1429e-07 | loss = 0.495993 |norm = 1.0118| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 3 | time = 0.1787s | lr = 2.8571e-07 | loss = 0.358145 |norm = 1.6058| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 4 | time = 0.2191s | lr = 3.5714e-07 | loss = 0.398114 |norm = 1.8334| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 5 | time = 0.1951s | lr = 4.2857e-07 | loss = 0.271266 |norm = 1.4436| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 6 | time = 0.1941s | lr = 5.0000e-07 | loss = 0.237859 |norm = 1.5047| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 7 | time = 0.1850s | lr = 5.7143e-07 | loss = 0.363855 |norm = 1.8806| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 8 | time = 0.2081s | lr = 6.4286e-07 | loss = 0.338151 |norm = 2.4073| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 9 | time = 0.1835s | lr = 7.1429e-07 | loss = 0.252560 |norm = 1.4880| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 10 | time = 0.1860s | lr = 7.8571e-07 | loss = 0.369541 |norm = 1.4840| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 11 | time = 0.1891s | lr = 8.5714e-07 | loss = 0.377380 |norm = 2.0812| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 12 | time = 0.1991s | lr = 9.2857e-07 | loss = 0.473236 |norm = 2.2075| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 13 | time = 0.1931s | lr = 1.0000e-06 | loss = 0.164314 |norm = 1.0947| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 14 | time = 0.2121s | lr = 1.0714e-06 | loss = 0.371910 |norm = 1.5329| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 15 | time = 0.1821s | lr = 1.1429e-06 | loss = 0.121785 |norm = 1.6028| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 16 | time = 0.1891s | lr = 1.2143e-06 | loss = 0.209550 |norm = 2.0459| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 17 | time = 0.1926s | lr = 1.2857e-06 | loss = 0.352910 |norm = 3.1950| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 18 | time = 0.1940s | lr = 1.3571e-06 | loss = 0.291058 |norm = 2.3813| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 19 | time = 0.2913s | lr = 1.4286e-06 | loss = 0.427837 |norm = 2.4907| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 20 | time = 0.2031s | lr = 1.5000e-06 | loss = 0.383259 |norm = 1.7261| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 21 | time = 0.2031s | lr = 1.5714e-06 | loss = 0.380533 |norm = 1.3573| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 22 | time = 0.2001s | lr = 1.6429e-06 | loss = 0.524749 |norm = 2.3380| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 23 | time = 0.1931s | lr = 1.7143e-06 | loss = 0.361139 |norm = 2.4861| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 24 | time = 0.1936s | lr = 1.7857e-06 | loss = 0.463973 |norm = 1.7808| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 25 | time = 0.1811s | lr = 1.8571e-06 | loss = 0.250970 |norm = 1.8642| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 26 | time = 0.1911s | lr = 1.9286e-06 | loss = 0.338607 |norm = 1.5691| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 27 | time = 0.1881s | lr = 2.0000e-06 | loss = 0.341753 |norm = 1.7495| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 28 | time = 0.2130s | lr = 2.0714e-06 | loss = 0.259955 |norm = 2.0698| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 29 | time = 0.2001s | lr = 2.1429e-06 | loss = 0.275735 |norm = 1.5778| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 30 | time = 0.1961s | lr = 2.2143e-06 | loss = 0.312914 |norm = 1.1463| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 31 | time = 0.1830s | lr = 2.2857e-06 | loss = 0.284945 |norm = 1.1863| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 32 | time = 0.1903s | lr = 2.3571e-06 | loss = 0.365841 |norm = 1.8362| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 33 | time = 0.1931s | lr = 2.4286e-06 | loss = 0.254569 |norm = 1.4234| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 34 | time = 0.1941s | lr = 2.5000e-06 | loss = 0.327954 |norm = 1.6141| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 35 | time = 0.1931s | lr = 2.5714e-06 | loss = 0.318567 |norm = 1.5524| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 36 | time = 0.1941s | lr = 2.6429e-06 | loss = 0.513473 |norm = 2.2038| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 37 | time = 0.1841s | lr = 2.7143e-06 | loss = 0.229375 |norm = 1.5828| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 38 | time = 0.1880s | lr = 2.7857e-06 | loss = 0.381085 |norm = 2.2668| latest val loss = 0.5212| accuracy = 0.0000\n",
      "| step = 39 | time = 0.2456s | lr = 2.8571e-06 | loss = 0.297282 |norm = 1.9347| latest val loss = 0.5212| accuracy = 0.0000\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  2,  0,  0,  0,  0,  6,  0,  0,\n",
      "        27,  0, 11,  0,  0,  0,  0,  0,  0,  1, 10,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "tensor([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 10, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 10, 29, 29],\n",
      "       device='cuda:0')\n",
      "| step = 40 | time = 0.3569s | lr = 2.9286e-06 | loss = 0.387919 |norm = 2.0958| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 41 | time = 0.1901s | lr = 3.0000e-06 | loss = 0.252529 |norm = 1.6844| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 42 | time = 0.1891s | lr = 3.0714e-06 | loss = 0.337708 |norm = 1.7653| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 43 | time = 0.1911s | lr = 3.1429e-06 | loss = 0.467869 |norm = 2.1018| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 44 | time = 0.1941s | lr = 3.2143e-06 | loss = 0.473202 |norm = 1.7912| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 45 | time = 0.1850s | lr = 3.2857e-06 | loss = 0.102126 |norm = 1.1480| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 46 | time = 0.1831s | lr = 3.3571e-06 | loss = 0.068423 |norm = 1.1525| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 47 | time = 0.1948s | lr = 3.4286e-06 | loss = 0.244582 |norm = 1.4375| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 48 | time = 0.1991s | lr = 3.5000e-06 | loss = 0.266054 |norm = 1.2168| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 49 | time = 0.1861s | lr = 3.5714e-06 | loss = 0.198699 |norm = 1.2972| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 50 | time = 0.1926s | lr = 3.6429e-06 | loss = 0.258369 |norm = 1.3010| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 51 | time = 0.1951s | lr = 3.7143e-06 | loss = 0.366618 |norm = 1.7106| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 52 | time = 0.1895s | lr = 3.7857e-06 | loss = 0.139476 |norm = 1.4982| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 53 | time = 0.1841s | lr = 3.8571e-06 | loss = 0.591112 |norm = 1.9304| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 54 | time = 0.1901s | lr = 3.9286e-06 | loss = 0.355989 |norm = 2.0886| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 55 | time = 0.1887s | lr = 4.0000e-06 | loss = 0.404962 |norm = 1.4871| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 56 | time = 0.2221s | lr = 4.0714e-06 | loss = 0.277190 |norm = 1.7816| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 57 | time = 0.1991s | lr = 4.1429e-06 | loss = 0.277786 |norm = 1.5157| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 58 | time = 0.2081s | lr = 4.2143e-06 | loss = 0.448894 |norm = 1.9657| latest val loss = 0.5086| accuracy = 0.0000\n",
      "11\n",
      "| step = 59 | time = 0.1971s | lr = 4.2857e-06 | loss = 0.349299 |norm = 1.9327| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 60 | time = 0.2351s | lr = 4.3571e-06 | loss = 0.408599 |norm = 2.0856| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 61 | time = 0.2211s | lr = 4.4286e-06 | loss = 0.159963 |norm = 1.5652| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 62 | time = 0.2221s | lr = 4.5000e-06 | loss = 0.174215 |norm = 1.4661| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 63 | time = 0.2561s | lr = 4.5714e-06 | loss = 0.655096 |norm = 2.8897| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 64 | time = 0.2141s | lr = 4.6429e-06 | loss = 0.157116 |norm = 1.4262| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 65 | time = 0.2176s | lr = 4.7143e-06 | loss = 0.346688 |norm = 1.9907| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 66 | time = 0.2536s | lr = 4.7857e-06 | loss = 0.259447 |norm = 1.6707| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 67 | time = 0.2156s | lr = 4.8571e-06 | loss = 0.223321 |norm = 1.9451| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 68 | time = 0.2101s | lr = 4.9286e-06 | loss = 0.202809 |norm = 1.5517| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 69 | time = 0.2261s | lr = 5.0000e-06 | loss = 0.274104 |norm = 1.9641| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 70 | time = 0.2182s | lr = 5.0000e-06 | loss = 0.381546 |norm = 1.8919| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 71 | time = 0.2256s | lr = 4.9921e-06 | loss = 0.115772 |norm = 1.1404| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 72 | time = 0.2211s | lr = 4.9841e-06 | loss = 0.251639 |norm = 1.3038| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 73 | time = 0.2186s | lr = 4.9762e-06 | loss = 0.167322 |norm = 1.1398| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 74 | time = 0.2076s | lr = 4.9683e-06 | loss = 0.492306 |norm = 1.4056| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 75 | time = 0.2291s | lr = 4.9603e-06 | loss = 0.260831 |norm = 1.6685| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 76 | time = 0.2171s | lr = 4.9524e-06 | loss = 0.201592 |norm = 1.6125| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 77 | time = 0.2181s | lr = 4.9444e-06 | loss = 0.369943 |norm = 1.4808| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 78 | time = 0.2133s | lr = 4.9365e-06 | loss = 0.401689 |norm = 1.9091| latest val loss = 0.5086| accuracy = 0.0000\n",
      "| step = 79 | time = 0.2542s | lr = 4.9286e-06 | loss = 0.552569 |norm = 2.1536| latest val loss = 0.5086| accuracy = 0.0000\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0, 21,  0,  0,  0,  0,  0,  0,  0,  0,  0, 13,\n",
      "         0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  3],\n",
      "       device='cuda:0')\n",
      "tensor([10, 28, 28, 28, 28, 28, 10, 10, 28, 28, 10, 28, 28, 10, 28, 10, 10, 10,\n",
      "        10, 10, 28, 10, 28, 28, 28, 10, 10, 10, 10, 28, 28, 10],\n",
      "       device='cuda:0')\n",
      "| step = 80 | time = 0.3887s | lr = 4.9206e-06 | loss = 0.366183 |norm = 1.9659| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 81 | time = 0.2226s | lr = 4.9127e-06 | loss = 0.272964 |norm = 1.5786| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 82 | time = 0.4163s | lr = 4.9048e-06 | loss = 0.376913 |norm = 1.8345| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 83 | time = 0.2281s | lr = 4.8968e-06 | loss = 0.397626 |norm = 1.8742| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 84 | time = 0.2251s | lr = 4.8889e-06 | loss = 0.354920 |norm = 1.7172| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 85 | time = 0.2211s | lr = 4.8810e-06 | loss = 0.357957 |norm = 1.4876| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 86 | time = 0.2281s | lr = 4.8730e-06 | loss = 0.458103 |norm = 1.8247| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 87 | time = 0.2286s | lr = 4.8651e-06 | loss = 0.262575 |norm = 1.5279| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 88 | time = 0.2263s | lr = 4.8571e-06 | loss = 0.326962 |norm = 1.6616| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 89 | time = 0.2131s | lr = 4.8492e-06 | loss = 0.496679 |norm = 2.0968| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 90 | time = 0.2131s | lr = 4.8413e-06 | loss = 0.458163 |norm = 2.3654| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 91 | time = 0.2211s | lr = 4.8333e-06 | loss = 0.422632 |norm = 1.9925| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 92 | time = 0.2234s | lr = 4.8254e-06 | loss = 0.197759 |norm = 1.8127| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 93 | time = 0.2121s | lr = 4.8175e-06 | loss = 0.278503 |norm = 2.5111| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 94 | time = 0.2201s | lr = 4.8095e-06 | loss = 0.355086 |norm = 2.3182| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 95 | time = 0.2211s | lr = 4.8016e-06 | loss = 0.478822 |norm = 2.5182| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 96 | time = 0.2176s | lr = 4.7937e-06 | loss = 0.314185 |norm = 1.7657| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 97 | time = 0.2101s | lr = 4.7857e-06 | loss = 0.275505 |norm = 1.7453| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 98 | time = 0.2170s | lr = 4.7778e-06 | loss = 0.159300 |norm = 1.8358| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 99 | time = 0.2490s | lr = 4.7698e-06 | loss = 0.337458 |norm = 1.5403| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 100 | time = 0.2261s | lr = 4.7619e-06 | loss = 0.326341 |norm = 1.6960| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 101 | time = 0.2161s | lr = 4.7540e-06 | loss = 0.393550 |norm = 2.0638| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 102 | time = 0.2166s | lr = 4.7460e-06 | loss = 0.260389 |norm = 1.9209| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 103 | time = 0.2241s | lr = 4.7381e-06 | loss = 0.469335 |norm = 2.6659| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 104 | time = 0.2806s | lr = 4.7302e-06 | loss = 0.278022 |norm = 2.0883| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 105 | time = 0.2206s | lr = 4.7222e-06 | loss = 0.200594 |norm = 1.5976| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 106 | time = 0.2171s | lr = 4.7143e-06 | loss = 0.405505 |norm = 2.0287| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 107 | time = 0.2311s | lr = 4.7063e-06 | loss = 0.394503 |norm = 2.0224| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 108 | time = 0.2271s | lr = 4.6984e-06 | loss = 0.303763 |norm = 1.5098| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 109 | time = 0.2181s | lr = 4.6905e-06 | loss = 0.219526 |norm = 1.6638| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 110 | time = 0.2266s | lr = 4.6825e-06 | loss = 0.461232 |norm = 2.7588| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 111 | time = 0.2181s | lr = 4.6746e-06 | loss = 0.341965 |norm = 2.8001| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 112 | time = 0.2351s | lr = 4.6667e-06 | loss = 0.215614 |norm = 2.1097| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 113 | time = 0.2259s | lr = 4.6587e-06 | loss = 0.308660 |norm = 1.9777| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 114 | time = 0.2168s | lr = 4.6508e-06 | loss = 0.175071 |norm = 2.0572| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 115 | time = 0.2227s | lr = 4.6429e-06 | loss = 0.469785 |norm = 2.4837| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 116 | time = 0.2158s | lr = 4.6349e-06 | loss = 0.360033 |norm = 1.2876| latest val loss = 0.3815| accuracy = 0.0312\n",
      "10\n",
      "| step = 117 | time = 0.2141s | lr = 4.6270e-06 | loss = 0.248385 |norm = 1.5043| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 118 | time = 0.2596s | lr = 4.6190e-06 | loss = 0.223754 |norm = 1.8526| latest val loss = 0.3815| accuracy = 0.0312\n",
      "| step = 119 | time = 0.2526s | lr = 4.6111e-06 | loss = 0.366988 |norm = 2.1933| latest val loss = 0.3815| accuracy = 0.0312\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0, 27, 10,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  0,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "tensor([10, 10, 30, 10,  6, 10,  6, 10, 10, 10,  6, 10,  6, 10, 10, 10, 10, 30,\n",
      "        10, 10,  6, 10, 10, 10, 10,  6, 10, 10, 10, 10,  6,  6],\n",
      "       device='cuda:0')\n",
      "| step = 120 | time = 0.4241s | lr = 4.6032e-06 | loss = 0.468903 |norm = 2.2407| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 121 | time = 0.2471s | lr = 4.5952e-06 | loss = 0.411034 |norm = 2.3622| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 122 | time = 0.2531s | lr = 4.5873e-06 | loss = 0.295089 |norm = 1.5424| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 123 | time = 0.2486s | lr = 4.5794e-06 | loss = 0.319790 |norm = 2.4544| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 124 | time = 0.2611s | lr = 4.5714e-06 | loss = 0.432617 |norm = 2.1485| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 125 | time = 0.2441s | lr = 4.5635e-06 | loss = 0.201201 |norm = 1.6498| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 126 | time = 0.2466s | lr = 4.5556e-06 | loss = 0.376960 |norm = 1.6024| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 127 | time = 0.2521s | lr = 4.5476e-06 | loss = 0.503687 |norm = 2.3281| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 128 | time = 0.2446s | lr = 4.5397e-06 | loss = 0.345218 |norm = 2.1585| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 129 | time = 0.4191s | lr = 4.5317e-06 | loss = 0.389425 |norm = 3.1911| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 130 | time = 0.2396s | lr = 4.5238e-06 | loss = 0.304796 |norm = 2.2497| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 131 | time = 0.2501s | lr = 4.5159e-06 | loss = 0.283039 |norm = 2.4131| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 132 | time = 0.2531s | lr = 4.5079e-06 | loss = 0.291364 |norm = 2.5171| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 133 | time = 0.2566s | lr = 4.5000e-06 | loss = 0.374473 |norm = 3.0276| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 134 | time = 0.2571s | lr = 4.4921e-06 | loss = 0.255547 |norm = 1.8482| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 135 | time = 0.2584s | lr = 4.4841e-06 | loss = 0.282586 |norm = 2.3141| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 136 | time = 0.2466s | lr = 4.4762e-06 | loss = 0.163808 |norm = 1.5346| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 137 | time = 0.2380s | lr = 4.4683e-06 | loss = 0.285506 |norm = 2.2624| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 138 | time = 0.2431s | lr = 4.4603e-06 | loss = 0.391980 |norm = 1.4888| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 139 | time = 0.2464s | lr = 4.4524e-06 | loss = 0.504803 |norm = 1.6350| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 140 | time = 0.2756s | lr = 4.4444e-06 | loss = 0.316040 |norm = 1.4444| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 141 | time = 0.2381s | lr = 4.4365e-06 | loss = 0.241139 |norm = 1.9766| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 142 | time = 0.2501s | lr = 4.4286e-06 | loss = 0.398360 |norm = 2.6408| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 143 | time = 0.2466s | lr = 4.4206e-06 | loss = 0.429221 |norm = 3.4388| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 144 | time = 0.2421s | lr = 4.4127e-06 | loss = 0.336493 |norm = 1.9129| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 145 | time = 0.2401s | lr = 4.4048e-06 | loss = 0.250121 |norm = 1.3615| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 146 | time = 0.2516s | lr = 4.3968e-06 | loss = 0.326625 |norm = 1.6758| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 147 | time = 0.2636s | lr = 4.3889e-06 | loss = 0.193474 |norm = 1.4121| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 148 | time = 0.2566s | lr = 4.3810e-06 | loss = 0.564953 |norm = 2.4699| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 149 | time = 0.2442s | lr = 4.3730e-06 | loss = 0.188316 |norm = 2.6533| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 150 | time = 0.2881s | lr = 4.3651e-06 | loss = 0.311305 |norm = 1.9285| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 151 | time = 0.4336s | lr = 4.3571e-06 | loss = 0.536839 |norm = 2.3946| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 152 | time = 0.2446s | lr = 4.3492e-06 | loss = 0.228961 |norm = 2.3739| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 153 | time = 0.2451s | lr = 4.3413e-06 | loss = 0.161297 |norm = 2.2035| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 154 | time = 0.2586s | lr = 4.3333e-06 | loss = 0.237068 |norm = 1.7431| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 155 | time = 0.2461s | lr = 4.3254e-06 | loss = 0.306562 |norm = 1.8070| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 156 | time = 0.2561s | lr = 4.3175e-06 | loss = 0.226431 |norm = 1.5896| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 157 | time = 0.2406s | lr = 4.3095e-06 | loss = 0.383028 |norm = 1.9569| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 158 | time = 0.2526s | lr = 4.3016e-06 | loss = 0.330199 |norm = 2.1339| latest val loss = 0.0844| accuracy = 0.1250\n",
      "| step = 159 | time = 0.2411s | lr = 4.2937e-06 | loss = 0.194857 |norm = 1.7878| latest val loss = 0.0844| accuracy = 0.1250\n",
      "tensor([ 0,  0,  0, 10,  0,  0,  0,  5,  0,  0,  6,  0,  0,  0,  0, 10, 28,  0,\n",
      "         0,  0,  0,  0,  0,  0, 10, 10,  0,  0,  3,  0,  0,  0],\n",
      "       device='cuda:0')\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "       device='cuda:0')\n",
      "| step = 160 | time = 0.4271s | lr = 4.2857e-06 | loss = 0.347798 |norm = 2.1201| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 161 | time = 0.2594s | lr = 4.2778e-06 | loss = 0.382272 |norm = 2.5440| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 162 | time = 0.2511s | lr = 4.2698e-06 | loss = 0.101609 |norm = 1.1591| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 163 | time = 0.2501s | lr = 4.2619e-06 | loss = 0.189404 |norm = 1.1662| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 164 | time = 0.2561s | lr = 4.2540e-06 | loss = 0.416380 |norm = 1.7110| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 165 | time = 0.2501s | lr = 4.2460e-06 | loss = 0.212345 |norm = 1.6185| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 166 | time = 0.2401s | lr = 4.2381e-06 | loss = 0.224127 |norm = 2.0453| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 167 | time = 0.2536s | lr = 4.2302e-06 | loss = 0.285895 |norm = 1.3758| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 168 | time = 0.2395s | lr = 4.2222e-06 | loss = 0.284396 |norm = 1.6549| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 169 | time = 0.4121s | lr = 4.2143e-06 | loss = 0.298572 |norm = 1.6846| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 170 | time = 0.2501s | lr = 4.2063e-06 | loss = 0.219810 |norm = 1.7953| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 171 | time = 0.2501s | lr = 4.1984e-06 | loss = 0.375513 |norm = 1.4697| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 172 | time = 0.2436s | lr = 4.1905e-06 | loss = 0.398744 |norm = 2.5369| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 173 | time = 0.2396s | lr = 4.1825e-06 | loss = 0.323398 |norm = 2.1513| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 174 | time = 0.2461s | lr = 4.1746e-06 | loss = 0.155659 |norm = 1.6790| latest val loss = 0.2401| accuracy = 0.1250\n",
      "9\n",
      "| step = 175 | time = 0.2481s | lr = 4.1667e-06 | loss = 0.331675 |norm = 1.7564| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 176 | time = 0.2836s | lr = 4.1587e-06 | loss = 0.449181 |norm = 2.1483| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 177 | time = 0.3451s | lr = 4.1508e-06 | loss = 0.225765 |norm = 1.6625| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 178 | time = 0.2816s | lr = 4.1429e-06 | loss = 0.326058 |norm = 2.1884| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 179 | time = 0.3036s | lr = 4.1349e-06 | loss = 0.399932 |norm = 2.3966| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 180 | time = 0.2916s | lr = 4.1270e-06 | loss = 0.249254 |norm = 1.8106| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 181 | time = 0.2816s | lr = 4.1190e-06 | loss = 0.277093 |norm = 1.6535| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 182 | time = 0.2876s | lr = 4.1111e-06 | loss = 0.253295 |norm = 1.6436| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 183 | time = 0.3176s | lr = 4.1032e-06 | loss = 0.184345 |norm = 1.7904| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 184 | time = 0.2906s | lr = 4.0952e-06 | loss = 0.110344 |norm = 1.4053| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 185 | time = 0.4293s | lr = 4.0873e-06 | loss = 0.291336 |norm = 2.0304| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 186 | time = 0.2731s | lr = 4.0794e-06 | loss = 0.310800 |norm = 1.6198| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 187 | time = 0.2756s | lr = 4.0714e-06 | loss = 0.408351 |norm = 1.8048| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 188 | time = 0.2902s | lr = 4.0635e-06 | loss = 0.269515 |norm = 1.6751| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 189 | time = 0.2866s | lr = 4.0556e-06 | loss = 0.370865 |norm = 1.8774| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 190 | time = 0.2756s | lr = 4.0476e-06 | loss = 0.100022 |norm = 1.2135| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 191 | time = 0.2816s | lr = 4.0397e-06 | loss = 0.337742 |norm = 1.6389| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 192 | time = 0.2802s | lr = 4.0317e-06 | loss = 0.269949 |norm = 1.6480| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 193 | time = 0.2786s | lr = 4.0238e-06 | loss = 0.382901 |norm = 2.1979| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 194 | time = 0.2876s | lr = 4.0159e-06 | loss = 0.135705 |norm = 1.4680| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 195 | time = 0.2826s | lr = 4.0079e-06 | loss = 0.341350 |norm = 2.0989| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 196 | time = 0.2726s | lr = 4.0000e-06 | loss = 0.240878 |norm = 2.1190| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 197 | time = 0.2766s | lr = 3.9921e-06 | loss = 0.231362 |norm = 2.0761| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 198 | time = 0.2781s | lr = 3.9841e-06 | loss = 0.258784 |norm = 1.8063| latest val loss = 0.2401| accuracy = 0.1250\n",
      "| step = 199 | time = 0.2761s | lr = 3.9762e-06 | loss = 0.401663 |norm = 1.8205| latest val loss = 0.2401| accuracy = 0.1250\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10],\n",
      "       device='cuda:0')\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 30, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 30, 10, 10, 30, 10, 10, 10],\n",
      "       device='cuda:0')\n",
      "| step = 200 | time = 0.4426s | lr = 3.9683e-06 | loss = 0.259499 |norm = 2.5588| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 201 | time = 0.4916s | lr = 3.9603e-06 | loss = 0.198998 |norm = 1.9938| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 202 | time = 0.2726s | lr = 3.9524e-06 | loss = 0.362561 |norm = 2.2400| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 203 | time = 0.2826s | lr = 3.9444e-06 | loss = 0.246176 |norm = 2.3258| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 204 | time = 0.2846s | lr = 3.9365e-06 | loss = 0.270155 |norm = 2.1164| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 205 | time = 0.2831s | lr = 3.9286e-06 | loss = 0.300810 |norm = 2.2957| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 206 | time = 0.2741s | lr = 3.9206e-06 | loss = 0.180944 |norm = 1.5032| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 207 | time = 0.2861s | lr = 3.9127e-06 | loss = 0.229299 |norm = 1.8190| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 208 | time = 0.2995s | lr = 3.9048e-06 | loss = 0.164231 |norm = 1.2837| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 209 | time = 0.2836s | lr = 3.8968e-06 | loss = 0.176398 |norm = 1.6802| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 210 | time = 0.2906s | lr = 3.8889e-06 | loss = 0.262950 |norm = 1.8580| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 211 | time = 0.2796s | lr = 3.8810e-06 | loss = 0.360651 |norm = 2.1418| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 212 | time = 0.3121s | lr = 3.8730e-06 | loss = 0.239676 |norm = 1.8508| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 213 | time = 0.2906s | lr = 3.8651e-06 | loss = 0.274748 |norm = 1.9501| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 214 | time = 0.3046s | lr = 3.8571e-06 | loss = 0.087367 |norm = 1.0479| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 215 | time = 0.3066s | lr = 3.8492e-06 | loss = 0.392000 |norm = 1.6114| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 216 | time = 0.2941s | lr = 3.8413e-06 | loss = 0.461957 |norm = 2.1504| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 217 | time = 0.2726s | lr = 3.8333e-06 | loss = 0.240335 |norm = 1.6606| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 218 | time = 0.2806s | lr = 3.8254e-06 | loss = 0.381090 |norm = 1.9470| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 219 | time = 0.2786s | lr = 3.8175e-06 | loss = 0.397045 |norm = 2.7156| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 220 | time = 0.3086s | lr = 3.8095e-06 | loss = 0.302361 |norm = 2.7396| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 221 | time = 0.2896s | lr = 3.8016e-06 | loss = 0.242934 |norm = 1.9988| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 222 | time = 0.2746s | lr = 3.7937e-06 | loss = 0.317908 |norm = 2.4685| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 223 | time = 0.2806s | lr = 3.7857e-06 | loss = 0.387429 |norm = 2.6214| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 224 | time = 0.2816s | lr = 3.7778e-06 | loss = 0.306321 |norm = 2.0736| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 225 | time = 0.3206s | lr = 3.7698e-06 | loss = 0.299661 |norm = 1.8337| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 226 | time = 0.2726s | lr = 3.7619e-06 | loss = 0.218053 |norm = 1.6703| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 227 | time = 0.2732s | lr = 3.7540e-06 | loss = 0.226861 |norm = 1.3514| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 228 | time = 0.2876s | lr = 3.7460e-06 | loss = 0.283569 |norm = 1.8194| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 229 | time = 0.2826s | lr = 3.7381e-06 | loss = 0.329148 |norm = 1.6600| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 230 | time = 0.2756s | lr = 3.7302e-06 | loss = 0.242693 |norm = 2.0277| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 231 | time = 0.2691s | lr = 3.7222e-06 | loss = 0.406058 |norm = 2.5023| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 232 | time = 0.2861s | lr = 3.7143e-06 | loss = 0.328347 |norm = 2.2912| latest val loss = 0.0616| accuracy = 0.0312\n",
      "8\n",
      "| step = 233 | time = 0.2836s | lr = 3.7063e-06 | loss = 0.324067 |norm = 1.9409| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 234 | time = 0.3106s | lr = 3.6984e-06 | loss = 0.216845 |norm = 1.3750| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 235 | time = 0.3042s | lr = 3.6905e-06 | loss = 0.369800 |norm = 1.9700| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 236 | time = 0.3136s | lr = 3.6825e-06 | loss = 0.278943 |norm = 1.8478| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 237 | time = 0.3251s | lr = 3.6746e-06 | loss = 0.264928 |norm = 1.8822| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 238 | time = 0.3266s | lr = 3.6667e-06 | loss = 0.162934 |norm = 1.4963| latest val loss = 0.0616| accuracy = 0.0312\n",
      "| step = 239 | time = 0.3096s | lr = 3.6587e-06 | loss = 0.187659 |norm = 1.2859| latest val loss = 0.0616| accuracy = 0.0312\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  2,  0,  0, 10,  5,  0,\n",
      "         0, 10,  0, 21,  0,  0, 29,  0,  0,  0,  2, 10,  0,  0],\n",
      "       device='cuda:0')\n",
      "tensor([10,  6,  6, 10, 10, 10, 10, 10, 10,  6, 10,  6, 10,  6,  6, 10,  6, 10,\n",
      "         6,  6,  6, 10,  6, 10, 10, 10,  6,  6,  6, 10,  6, 10],\n",
      "       device='cuda:0')\n",
      "| step = 240 | time = 0.4904s | lr = 3.6508e-06 | loss = 0.350470 |norm = 2.2049| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 241 | time = 0.3034s | lr = 3.6429e-06 | loss = 0.362340 |norm = 1.9105| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 242 | time = 0.3136s | lr = 3.6349e-06 | loss = 0.282494 |norm = 1.9301| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 243 | time = 0.3096s | lr = 3.6270e-06 | loss = 0.261961 |norm = 1.8504| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 244 | time = 0.3016s | lr = 3.6190e-06 | loss = 0.326447 |norm = 1.1859| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 245 | time = 0.3036s | lr = 3.6111e-06 | loss = 0.213238 |norm = 1.3932| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 246 | time = 0.3076s | lr = 3.6032e-06 | loss = 0.275455 |norm = 1.7072| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 247 | time = 0.4696s | lr = 3.5952e-06 | loss = 0.311615 |norm = 1.7366| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 248 | time = 0.3171s | lr = 3.5873e-06 | loss = 0.207385 |norm = 1.7763| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 249 | time = 0.3106s | lr = 3.5794e-06 | loss = 0.329910 |norm = 1.8985| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 250 | time = 0.3250s | lr = 3.5714e-06 | loss = 0.401510 |norm = 2.0123| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 251 | time = 0.3125s | lr = 3.5635e-06 | loss = 0.223035 |norm = 1.9494| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 252 | time = 0.3136s | lr = 3.5556e-06 | loss = 0.078615 |norm = 1.1288| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 253 | time = 0.3105s | lr = 3.5476e-06 | loss = 0.210576 |norm = 1.5657| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 254 | time = 0.3166s | lr = 3.5397e-06 | loss = 0.371873 |norm = 2.1261| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 255 | time = 0.3306s | lr = 3.5317e-06 | loss = 0.402477 |norm = 2.7077| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 256 | time = 0.3061s | lr = 3.5238e-06 | loss = 0.078735 |norm = 1.1906| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 257 | time = 0.3106s | lr = 3.5159e-06 | loss = 0.246286 |norm = 1.5073| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 258 | time = 0.3036s | lr = 3.5079e-06 | loss = 0.288127 |norm = 1.9719| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 259 | time = 0.3156s | lr = 3.5000e-06 | loss = 0.291726 |norm = 2.0658| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 260 | time = 0.3056s | lr = 3.4921e-06 | loss = 0.392893 |norm = 2.3017| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 261 | time = 0.3146s | lr = 3.4841e-06 | loss = 0.135192 |norm = 1.4932| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 262 | time = 0.3046s | lr = 3.4762e-06 | loss = 0.227356 |norm = 1.9017| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 263 | time = 0.3436s | lr = 3.4683e-06 | loss = 0.334876 |norm = 1.9573| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 264 | time = 0.3116s | lr = 3.4603e-06 | loss = 0.309979 |norm = 2.1682| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 265 | time = 0.3126s | lr = 3.4524e-06 | loss = 0.207471 |norm = 2.1108| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 266 | time = 0.3365s | lr = 3.4444e-06 | loss = 0.379123 |norm = 2.3050| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 267 | time = 0.3101s | lr = 3.4365e-06 | loss = 0.247029 |norm = 2.2556| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 268 | time = 0.3226s | lr = 3.4286e-06 | loss = 0.352662 |norm = 2.1705| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 269 | time = 0.3155s | lr = 3.4206e-06 | loss = 0.227702 |norm = 1.5515| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 270 | time = 0.3395s | lr = 3.4127e-06 | loss = 0.206170 |norm = 2.1474| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 271 | time = 0.3106s | lr = 3.4048e-06 | loss = 0.375229 |norm = 2.7473| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 272 | time = 0.3131s | lr = 3.3968e-06 | loss = 0.273272 |norm = 2.1854| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 273 | time = 0.3036s | lr = 3.3889e-06 | loss = 0.497273 |norm = 2.9036| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 274 | time = 0.3136s | lr = 3.3810e-06 | loss = 0.220371 |norm = 1.9998| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 275 | time = 0.3226s | lr = 3.3730e-06 | loss = 0.388658 |norm = 2.3860| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 276 | time = 0.3301s | lr = 3.3651e-06 | loss = 0.388950 |norm = 2.4150| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 277 | time = 0.3216s | lr = 3.3571e-06 | loss = 0.166853 |norm = 1.5879| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 278 | time = 0.3166s | lr = 3.3492e-06 | loss = 0.197722 |norm = 1.6872| latest val loss = 0.3644| accuracy = 0.0938\n",
      "| step = 279 | time = 0.3146s | lr = 3.3413e-06 | loss = 0.396948 |norm = 3.0395| latest val loss = 0.3644| accuracy = 0.0938\n",
      "tensor([29,  0,  0,  0,  0,  0,  0, 11,  0, 10,  0, 20,  0,  0,  0,  0, 22, 29,\n",
      "         0,  0,  3,  0,  0,  1,  0,  0, 10,  0,  0,  0, 10,  0],\n",
      "       device='cuda:0')\n",
      "tensor([10, 10, 10, 10, 10,  6, 10, 10, 10, 10, 10, 29, 10, 10, 10, 10, 10, 10,\n",
      "         6, 10, 10, 10, 10, 10, 10, 10, 29, 10, 10, 10, 10, 10],\n",
      "       device='cuda:0')\n",
      "| step = 280 | time = 0.4696s | lr = 3.3333e-06 | loss = 0.245185 |norm = 2.6240| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 281 | time = 0.3096s | lr = 3.3254e-06 | loss = 0.289787 |norm = 1.9324| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 282 | time = 0.3092s | lr = 3.3175e-06 | loss = 0.394200 |norm = 1.9982| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 283 | time = 0.3186s | lr = 3.3095e-06 | loss = 0.248911 |norm = 1.7804| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 284 | time = 0.3221s | lr = 3.3016e-06 | loss = 0.308103 |norm = 1.6296| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 285 | time = 0.3016s | lr = 3.2937e-06 | loss = 0.360243 |norm = 1.9395| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 286 | time = 0.2986s | lr = 3.2857e-06 | loss = 0.177963 |norm = 1.3742| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 287 | time = 0.3054s | lr = 3.2778e-06 | loss = 0.227819 |norm = 1.5827| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 288 | time = 0.3106s | lr = 3.2698e-06 | loss = 0.440605 |norm = 1.8330| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 289 | time = 0.3235s | lr = 3.2619e-06 | loss = 0.172999 |norm = 1.5573| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 290 | time = 0.5081s | lr = 3.2540e-06 | loss = 0.170846 |norm = 2.0551| latest val loss = 0.4708| accuracy = 0.0625\n",
      "7\n",
      "| step = 291 | time = 0.3116s | lr = 3.2460e-06 | loss = 0.287118 |norm = 1.8224| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 292 | time = 0.3400s | lr = 3.2381e-06 | loss = 0.347217 |norm = 2.4502| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 293 | time = 0.3391s | lr = 3.2302e-06 | loss = 0.221385 |norm = 2.4233| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 294 | time = 0.3367s | lr = 3.2222e-06 | loss = 0.338342 |norm = 2.6277| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 295 | time = 0.3381s | lr = 3.2143e-06 | loss = 0.434327 |norm = 2.7364| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 296 | time = 0.3411s | lr = 3.2063e-06 | loss = 0.456939 |norm = 2.2115| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 297 | time = 0.3511s | lr = 3.1984e-06 | loss = 0.286085 |norm = 2.3619| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 298 | time = 0.3396s | lr = 3.1905e-06 | loss = 0.235806 |norm = 2.2800| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 299 | time = 0.3386s | lr = 3.1825e-06 | loss = 0.264865 |norm = 2.1648| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 300 | time = 0.3771s | lr = 3.1746e-06 | loss = 0.322351 |norm = 2.1019| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 301 | time = 0.3421s | lr = 3.1667e-06 | loss = 0.223810 |norm = 1.3687| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 302 | time = 0.3481s | lr = 3.1587e-06 | loss = 0.185127 |norm = 2.0850| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 303 | time = 0.3421s | lr = 3.1508e-06 | loss = 0.236132 |norm = 1.8579| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 304 | time = 0.3541s | lr = 3.1429e-06 | loss = 0.201691 |norm = 1.9598| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 305 | time = 0.3406s | lr = 3.1349e-06 | loss = 0.305949 |norm = 1.5991| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 306 | time = 0.3446s | lr = 3.1270e-06 | loss = 0.330614 |norm = 1.9108| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 307 | time = 0.3466s | lr = 3.1190e-06 | loss = 0.233978 |norm = 2.5416| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 308 | time = 0.3376s | lr = 3.1111e-06 | loss = 0.205423 |norm = 2.7934| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 309 | time = 0.3486s | lr = 3.1032e-06 | loss = 0.111809 |norm = 1.6157| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 310 | time = 0.3426s | lr = 3.0952e-06 | loss = 0.277656 |norm = 2.1900| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 311 | time = 0.3506s | lr = 3.0873e-06 | loss = 0.225368 |norm = 2.1754| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 312 | time = 0.5196s | lr = 3.0794e-06 | loss = 0.195419 |norm = 1.9518| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 313 | time = 0.3466s | lr = 3.0714e-06 | loss = 0.159151 |norm = 1.5725| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 314 | time = 0.3356s | lr = 3.0635e-06 | loss = 0.219489 |norm = 1.8066| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 315 | time = 0.3561s | lr = 3.0556e-06 | loss = 0.371746 |norm = 2.0454| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 316 | time = 0.3481s | lr = 3.0476e-06 | loss = 0.101060 |norm = 1.4323| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 317 | time = 0.4051s | lr = 3.0397e-06 | loss = 0.271507 |norm = 1.6801| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 318 | time = 0.3491s | lr = 3.0317e-06 | loss = 0.237781 |norm = 1.4914| latest val loss = 0.4708| accuracy = 0.0625\n",
      "| step = 319 | time = 0.3370s | lr = 3.0238e-06 | loss = 0.196099 |norm = 1.6332| latest val loss = 0.4708| accuracy = 0.0625\n",
      "tensor([ 0,  0,  3,  0,  5, 20,  0,  0,  0, 10,  0,  0,  0,  0,  0,  0,  5,  0,\n",
      "         0,  0,  0,  0, 21,  0,  0,  0,  0,  0,  0, 11,  0,  0],\n",
      "       device='cuda:0')\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "       device='cuda:0')\n",
      "| step = 320 | time = 0.5148s | lr = 3.0159e-06 | loss = 0.170559 |norm = 1.5057| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 321 | time = 0.3631s | lr = 3.0079e-06 | loss = 0.211615 |norm = 2.1121| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 322 | time = 0.3531s | lr = 3.0000e-06 | loss = 0.252977 |norm = 1.8633| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 323 | time = 0.3440s | lr = 2.9921e-06 | loss = 0.406147 |norm = 2.2604| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 324 | time = 0.3446s | lr = 2.9841e-06 | loss = 0.136106 |norm = 2.0665| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 325 | time = 0.3366s | lr = 2.9762e-06 | loss = 0.201138 |norm = 1.8262| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 326 | time = 0.3476s | lr = 2.9683e-06 | loss = 0.255096 |norm = 1.8560| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 327 | time = 0.3706s | lr = 2.9603e-06 | loss = 0.130160 |norm = 1.3336| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 328 | time = 0.3477s | lr = 2.9524e-06 | loss = 0.129303 |norm = 1.3746| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 329 | time = 0.5042s | lr = 2.9444e-06 | loss = 0.336385 |norm = 1.4342| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 330 | time = 0.3556s | lr = 2.9365e-06 | loss = 0.158789 |norm = 1.7791| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 331 | time = 0.3591s | lr = 2.9286e-06 | loss = 0.311041 |norm = 2.7661| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 332 | time = 0.3530s | lr = 2.9206e-06 | loss = 0.165879 |norm = 2.0717| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 333 | time = 0.3466s | lr = 2.9127e-06 | loss = 0.450347 |norm = 3.1462| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 334 | time = 0.3426s | lr = 2.9048e-06 | loss = 0.142996 |norm = 1.5596| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 335 | time = 0.3376s | lr = 2.8968e-06 | loss = 0.279980 |norm = 2.1310| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 336 | time = 0.3466s | lr = 2.8889e-06 | loss = 0.222845 |norm = 1.5789| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 337 | time = 0.3426s | lr = 2.8810e-06 | loss = 0.376370 |norm = 2.2761| latest val loss = 0.5074| accuracy = 0.0312\n",
      "| step = 338 | time = 0.3749s | lr = 2.8730e-06 | loss = 0.294752 |norm = 2.3176| latest val loss = 0.5074| accuracy = 0.0312\n"
     ]
    }
   ],
   "source": [
    "#model = bert_classifier(config).to(device)\n",
    "for step in range(max_steps):\n",
    "    last_step = (step == max_steps - 1)\n",
    "    t0 = time.time()\n",
    "    #optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    #validation loss\n",
    "    if step % 40 == 0 or last_step:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_acum = 0\n",
    "            val_accum = 0\n",
    "            val_loss_steps = 1\n",
    "            for val_steps in range(val_loss_steps):\n",
    "                x, y = next(iter(test_loader))\n",
    "                x,y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits,y)\n",
    "                values,ind = torch.max(logits,dim = 1)\n",
    "                print(y)\n",
    "                print(ind)\n",
    "                correct = np.sum((torch.eq(ind.to(\"cpu\"),y.to(\"cpu\")).numpy()))\n",
    "                correct_acum = correct_acum + correct\n",
    "                val_accum = val_accum + loss.detach()\n",
    "            accuracy = correct_acum / (val_loss_steps * B)\n",
    "            val_loss = val_accum / val_loss_steps\n",
    "    \n",
    "    #save checkpoint\n",
    "    if step % 500 == 0:\n",
    "        torch.save(model.state_dict(),f\"checkpoint{checkpoint}.pt\")\n",
    "        checkpoint = checkpoint+1\n",
    "\n",
    "    #minibatch process\n",
    "    loss_accum = 0\n",
    "    model.train()\n",
    "    for mini_step in range(grad_accum_steps):\n",
    "        x, y = next(iter(train_loader))\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits,y)\n",
    "        #print(f\"loss mini_step = {loss} | ministep = {mini_step}\")\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum = loss_accum + loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "    \n",
    "    a.check_unfreeze(step)\n",
    "    lr = get_lr(step)\n",
    "    parameter = disc_fine_tuning(lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group = parameter\n",
    "        \n",
    "    \n",
    "    optimizer.step()\n",
    "    print(f\"| step = {step} | time = {dt:.4f}s | lr = {lr:.4e} | loss = {loss_accum.item():.6f} |norm = {norm:.4f}| latest val loss = {val_loss:.4f}| accuracy = {accuracy:.4f}\")\n",
    "torch.save(model.state_dict(),\"Final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_bert.embeddings.word_embeddings.weight\n",
      "model_bert.embeddings.position_embeddings.weight\n",
      "model_bert.embeddings.token_type_embeddings.weight\n",
      "model_bert.embeddings.LayerNorm.weight\n",
      "model_bert.embeddings.LayerNorm.bias\n",
      "model_bert.encoder.layer.0.attention.self.query.weight\n",
      "model_bert.encoder.layer.0.attention.self.query.bias\n",
      "model_bert.encoder.layer.0.attention.self.key.weight\n",
      "model_bert.encoder.layer.0.attention.self.key.bias\n",
      "model_bert.encoder.layer.0.attention.self.value.weight\n",
      "model_bert.encoder.layer.0.attention.self.value.bias\n",
      "model_bert.encoder.layer.0.attention.output.dense.weight\n",
      "model_bert.encoder.layer.0.attention.output.dense.bias\n",
      "model_bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.0.intermediate.dense.weight\n",
      "model_bert.encoder.layer.0.intermediate.dense.bias\n",
      "model_bert.encoder.layer.0.output.dense.weight\n",
      "model_bert.encoder.layer.0.output.dense.bias\n",
      "model_bert.encoder.layer.0.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.0.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.1.attention.self.query.weight\n",
      "model_bert.encoder.layer.1.attention.self.query.bias\n",
      "model_bert.encoder.layer.1.attention.self.key.weight\n",
      "model_bert.encoder.layer.1.attention.self.key.bias\n",
      "model_bert.encoder.layer.1.attention.self.value.weight\n",
      "model_bert.encoder.layer.1.attention.self.value.bias\n",
      "model_bert.encoder.layer.1.attention.output.dense.weight\n",
      "model_bert.encoder.layer.1.attention.output.dense.bias\n",
      "model_bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.1.intermediate.dense.weight\n",
      "model_bert.encoder.layer.1.intermediate.dense.bias\n",
      "model_bert.encoder.layer.1.output.dense.weight\n",
      "model_bert.encoder.layer.1.output.dense.bias\n",
      "model_bert.encoder.layer.1.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.1.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.2.attention.self.query.weight\n",
      "model_bert.encoder.layer.2.attention.self.query.bias\n",
      "model_bert.encoder.layer.2.attention.self.key.weight\n",
      "model_bert.encoder.layer.2.attention.self.key.bias\n",
      "model_bert.encoder.layer.2.attention.self.value.weight\n",
      "model_bert.encoder.layer.2.attention.self.value.bias\n",
      "model_bert.encoder.layer.2.attention.output.dense.weight\n",
      "model_bert.encoder.layer.2.attention.output.dense.bias\n",
      "model_bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.2.intermediate.dense.weight\n",
      "model_bert.encoder.layer.2.intermediate.dense.bias\n",
      "model_bert.encoder.layer.2.output.dense.weight\n",
      "model_bert.encoder.layer.2.output.dense.bias\n",
      "model_bert.encoder.layer.2.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.2.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.3.attention.self.query.weight\n",
      "model_bert.encoder.layer.3.attention.self.query.bias\n",
      "model_bert.encoder.layer.3.attention.self.key.weight\n",
      "model_bert.encoder.layer.3.attention.self.key.bias\n",
      "model_bert.encoder.layer.3.attention.self.value.weight\n",
      "model_bert.encoder.layer.3.attention.self.value.bias\n",
      "model_bert.encoder.layer.3.attention.output.dense.weight\n",
      "model_bert.encoder.layer.3.attention.output.dense.bias\n",
      "model_bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.3.intermediate.dense.weight\n",
      "model_bert.encoder.layer.3.intermediate.dense.bias\n",
      "model_bert.encoder.layer.3.output.dense.weight\n",
      "model_bert.encoder.layer.3.output.dense.bias\n",
      "model_bert.encoder.layer.3.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.3.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.4.attention.self.query.weight\n",
      "model_bert.encoder.layer.4.attention.self.query.bias\n",
      "model_bert.encoder.layer.4.attention.self.key.weight\n",
      "model_bert.encoder.layer.4.attention.self.key.bias\n",
      "model_bert.encoder.layer.4.attention.self.value.weight\n",
      "model_bert.encoder.layer.4.attention.self.value.bias\n",
      "model_bert.encoder.layer.4.attention.output.dense.weight\n",
      "model_bert.encoder.layer.4.attention.output.dense.bias\n",
      "model_bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.4.intermediate.dense.weight\n",
      "model_bert.encoder.layer.4.intermediate.dense.bias\n",
      "model_bert.encoder.layer.4.output.dense.weight\n",
      "model_bert.encoder.layer.4.output.dense.bias\n",
      "model_bert.encoder.layer.4.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.4.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.5.attention.self.query.weight\n",
      "model_bert.encoder.layer.5.attention.self.query.bias\n",
      "model_bert.encoder.layer.5.attention.self.key.weight\n",
      "model_bert.encoder.layer.5.attention.self.key.bias\n",
      "model_bert.encoder.layer.5.attention.self.value.weight\n",
      "model_bert.encoder.layer.5.attention.self.value.bias\n",
      "model_bert.encoder.layer.5.attention.output.dense.weight\n",
      "model_bert.encoder.layer.5.attention.output.dense.bias\n",
      "model_bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.5.intermediate.dense.weight\n",
      "model_bert.encoder.layer.5.intermediate.dense.bias\n",
      "model_bert.encoder.layer.5.output.dense.weight\n",
      "model_bert.encoder.layer.5.output.dense.bias\n",
      "model_bert.encoder.layer.5.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.5.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.6.attention.self.query.weight\n",
      "model_bert.encoder.layer.6.attention.self.query.bias\n",
      "model_bert.encoder.layer.6.attention.self.key.weight\n",
      "model_bert.encoder.layer.6.attention.self.key.bias\n",
      "model_bert.encoder.layer.6.attention.self.value.weight\n",
      "model_bert.encoder.layer.6.attention.self.value.bias\n",
      "model_bert.encoder.layer.6.attention.output.dense.weight\n",
      "model_bert.encoder.layer.6.attention.output.dense.bias\n",
      "model_bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.6.intermediate.dense.weight\n",
      "model_bert.encoder.layer.6.intermediate.dense.bias\n",
      "model_bert.encoder.layer.6.output.dense.weight\n",
      "model_bert.encoder.layer.6.output.dense.bias\n",
      "model_bert.encoder.layer.6.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.6.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.7.attention.self.query.weight\n",
      "model_bert.encoder.layer.7.attention.self.query.bias\n",
      "model_bert.encoder.layer.7.attention.self.key.weight\n",
      "model_bert.encoder.layer.7.attention.self.key.bias\n",
      "model_bert.encoder.layer.7.attention.self.value.weight\n",
      "model_bert.encoder.layer.7.attention.self.value.bias\n",
      "model_bert.encoder.layer.7.attention.output.dense.weight\n",
      "model_bert.encoder.layer.7.attention.output.dense.bias\n",
      "model_bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.7.intermediate.dense.weight\n",
      "model_bert.encoder.layer.7.intermediate.dense.bias\n",
      "model_bert.encoder.layer.7.output.dense.weight\n",
      "model_bert.encoder.layer.7.output.dense.bias\n",
      "model_bert.encoder.layer.7.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.7.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.8.attention.self.query.weight\n",
      "model_bert.encoder.layer.8.attention.self.query.bias\n",
      "model_bert.encoder.layer.8.attention.self.key.weight\n",
      "model_bert.encoder.layer.8.attention.self.key.bias\n",
      "model_bert.encoder.layer.8.attention.self.value.weight\n",
      "model_bert.encoder.layer.8.attention.self.value.bias\n",
      "model_bert.encoder.layer.8.attention.output.dense.weight\n",
      "model_bert.encoder.layer.8.attention.output.dense.bias\n",
      "model_bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.8.intermediate.dense.weight\n",
      "model_bert.encoder.layer.8.intermediate.dense.bias\n",
      "model_bert.encoder.layer.8.output.dense.weight\n",
      "model_bert.encoder.layer.8.output.dense.bias\n",
      "model_bert.encoder.layer.8.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.8.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.9.attention.self.query.weight\n",
      "model_bert.encoder.layer.9.attention.self.query.bias\n",
      "model_bert.encoder.layer.9.attention.self.key.weight\n",
      "model_bert.encoder.layer.9.attention.self.key.bias\n",
      "model_bert.encoder.layer.9.attention.self.value.weight\n",
      "model_bert.encoder.layer.9.attention.self.value.bias\n",
      "model_bert.encoder.layer.9.attention.output.dense.weight\n",
      "model_bert.encoder.layer.9.attention.output.dense.bias\n",
      "model_bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.9.intermediate.dense.weight\n",
      "model_bert.encoder.layer.9.intermediate.dense.bias\n",
      "model_bert.encoder.layer.9.output.dense.weight\n",
      "model_bert.encoder.layer.9.output.dense.bias\n",
      "model_bert.encoder.layer.9.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.9.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.10.attention.self.query.weight\n",
      "model_bert.encoder.layer.10.attention.self.query.bias\n",
      "model_bert.encoder.layer.10.attention.self.key.weight\n",
      "model_bert.encoder.layer.10.attention.self.key.bias\n",
      "model_bert.encoder.layer.10.attention.self.value.weight\n",
      "model_bert.encoder.layer.10.attention.self.value.bias\n",
      "model_bert.encoder.layer.10.attention.output.dense.weight\n",
      "model_bert.encoder.layer.10.attention.output.dense.bias\n",
      "model_bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.10.intermediate.dense.weight\n",
      "model_bert.encoder.layer.10.intermediate.dense.bias\n",
      "model_bert.encoder.layer.10.output.dense.weight\n",
      "model_bert.encoder.layer.10.output.dense.bias\n",
      "model_bert.encoder.layer.10.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.10.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.11.attention.self.query.weight\n",
      "model_bert.encoder.layer.11.attention.self.query.bias\n",
      "model_bert.encoder.layer.11.attention.self.key.weight\n",
      "model_bert.encoder.layer.11.attention.self.key.bias\n",
      "model_bert.encoder.layer.11.attention.self.value.weight\n",
      "model_bert.encoder.layer.11.attention.self.value.bias\n",
      "model_bert.encoder.layer.11.attention.output.dense.weight\n",
      "model_bert.encoder.layer.11.attention.output.dense.bias\n",
      "model_bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model_bert.encoder.layer.11.intermediate.dense.weight\n",
      "model_bert.encoder.layer.11.intermediate.dense.bias\n",
      "model_bert.encoder.layer.11.output.dense.weight\n",
      "model_bert.encoder.layer.11.output.dense.bias\n",
      "model_bert.encoder.layer.11.output.LayerNorm.weight\n",
      "model_bert.encoder.layer.11.output.LayerNorm.bias\n",
      "model_bert.pooler.dense.weight\n",
      "model_bert.pooler.dense.bias\n",
      "lin.weight\n",
      "lin.bias\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
